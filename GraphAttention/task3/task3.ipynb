{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import scipy.sparse as sp \n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    #dataset=\"cora\"\n",
    "    dataset=\"citeseer\"\n",
    "    \n",
    "    path=\"./data/\"+dataset+\"/\" \n",
    "    \n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path,dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:,1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:,-1])\n",
    "\n",
    "    idx = np.array(idx_features_labels[:,0],dtype=np.dtype(str))\n",
    "    idx_map = {j: i for i,j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path,dataset), dtype=np.dtype(str))\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.dtype(str)).reshape(edges_unordered.shape)\n",
    "    \n",
    "    #edges = np.concatenate((edges,np.flip(edges,1)))\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T>adj) - adj.multiply(adj.T>adj)\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj+sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200,500)\n",
    "    idx_test = range(500,1500)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test \n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "\n",
    "    return correct / len(labels)\n",
    "\n",
    "def normalize_adj(mx): \n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    mx_to =  mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "    return mx_to\n",
    "\n",
    "def normalize_features(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx_to =  r_mat_inv.dot(mx) \n",
    "    return mx_to \n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i,:] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_features, out_features, alpha):\n",
    "        super(Attention, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Linear(in_features, out_features, bias = False)\n",
    "        self.a_T = nn.Linear(2 * out_features, 1, bias = False)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.xavier_uniform_(self.a_T.weight)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        N = h.size(0)\n",
    "        Wh = self.W(h) \n",
    "        \n",
    "        H1 = Wh.unsqueeze(1).repeat(1,N,1)\n",
    "        H2 = Wh.unsqueeze(0).repeat(N,1,1)\n",
    "        attn_input = torch.cat([H1, H2], dim = -1)\n",
    "\n",
    "        e = F.leaky_relu(self.a_T(attn_input).squeeze(-1), negative_slope = self.alpha)\n",
    "        \n",
    "        attn_mask = -1e18*torch.ones_like(e)\n",
    "        masked_e = torch.where(adj > 0, e, attn_mask)\n",
    "        attn_scores = F.softmax(masked_e, dim = -1) \n",
    "\n",
    "        h_prime = torch.mm(attn_scores, Wh) \n",
    "\n",
    "        return F.relu(h_prime) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.concat = concat\n",
    "        self.attentions = nn.ModuleList([Attention(in_features, out_features, alpha) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "\n",
    "        if self.concat :\n",
    "            # concatenate\n",
    "            outputs = []\n",
    "            for attention in self.attentions:\n",
    "                outputs.append(attention(input, adj))\n",
    "            \n",
    "            return torch.cat(outputs, dim = -1) \n",
    "\n",
    "        else :\n",
    "            # average\n",
    "            output = None\n",
    "            for attention in self.attentions:\n",
    "                if output == None:\n",
    "                    output = attention(input, adj)\n",
    "                else:\n",
    "                    output += attention(input, adj)\n",
    "            \n",
    "            return output/len(self.attentions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, F, H, C, dropout, alpha, K):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = GraphAttentionLayer(F, H, K, alpha)\n",
    "        self.layer2 = GraphAttentionLayer(K * H, C, 1, alpha, concat = False)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        x = self.dropout(F.relu(self.layer1(x, adj))) \n",
    "        return self.layer2(x, adj) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/350] train loss : 1.7917 | train acc 25.71% | val loss 1.7904 | val acc 35.33% | time 0.273s\n",
      "[2/350] train loss : 1.7899 | train acc 41.43% | val loss 1.7887 | val acc 44.00% | time 0.268s\n",
      "[3/350] train loss : 1.7875 | train acc 53.57% | val loss 1.7868 | val acc 46.33% | time 0.270s\n",
      "[4/350] train loss : 1.7849 | train acc 53.57% | val loss 1.7849 | val acc 46.00% | time 0.276s\n",
      "[5/350] train loss : 1.7815 | train acc 56.43% | val loss 1.7829 | val acc 46.33% | time 0.269s\n",
      "[6/350] train loss : 1.7786 | train acc 55.71% | val loss 1.7807 | val acc 47.67% | time 0.271s\n",
      "[7/350] train loss : 1.7768 | train acc 57.86% | val loss 1.7784 | val acc 50.00% | time 0.271s\n",
      "[8/350] train loss : 1.7719 | train acc 62.86% | val loss 1.7760 | val acc 54.33% | time 0.270s\n",
      "[9/350] train loss : 1.7698 | train acc 62.86% | val loss 1.7736 | val acc 57.67% | time 0.270s\n",
      "[10/350] train loss : 1.7642 | train acc 66.43% | val loss 1.7712 | val acc 59.67% | time 0.270s\n",
      "[11/350] train loss : 1.7639 | train acc 63.57% | val loss 1.7686 | val acc 59.67% | time 0.270s\n",
      "[12/350] train loss : 1.7596 | train acc 63.57% | val loss 1.7661 | val acc 60.67% | time 0.270s\n",
      "[13/350] train loss : 1.7519 | train acc 71.43% | val loss 1.7635 | val acc 61.00% | time 0.270s\n",
      "[14/350] train loss : 1.7509 | train acc 67.86% | val loss 1.7610 | val acc 61.33% | time 0.269s\n",
      "[15/350] train loss : 1.7477 | train acc 66.43% | val loss 1.7583 | val acc 61.67% | time 0.270s\n",
      "[16/350] train loss : 1.7439 | train acc 69.29% | val loss 1.7555 | val acc 62.00% | time 0.269s\n",
      "[17/350] train loss : 1.7414 | train acc 67.86% | val loss 1.7526 | val acc 63.33% | time 0.267s\n",
      "[18/350] train loss : 1.7366 | train acc 70.00% | val loss 1.7497 | val acc 63.00% | time 0.269s\n",
      "[19/350] train loss : 1.7343 | train acc 70.71% | val loss 1.7466 | val acc 63.00% | time 0.269s\n",
      "[20/350] train loss : 1.7276 | train acc 73.57% | val loss 1.7435 | val acc 63.67% | time 0.269s\n",
      "[21/350] train loss : 1.7267 | train acc 74.29% | val loss 1.7405 | val acc 64.00% | time 0.269s\n",
      "[22/350] train loss : 1.7208 | train acc 72.14% | val loss 1.7374 | val acc 64.00% | time 0.269s\n",
      "[23/350] train loss : 1.7125 | train acc 72.14% | val loss 1.7343 | val acc 64.00% | time 0.269s\n",
      "[24/350] train loss : 1.7156 | train acc 70.00% | val loss 1.7311 | val acc 64.33% | time 0.269s\n",
      "[25/350] train loss : 1.7071 | train acc 72.14% | val loss 1.7277 | val acc 65.33% | time 0.269s\n",
      "[26/350] train loss : 1.7054 | train acc 72.86% | val loss 1.7242 | val acc 65.33% | time 0.268s\n",
      "[27/350] train loss : 1.7033 | train acc 72.86% | val loss 1.7206 | val acc 66.00% | time 0.269s\n",
      "[28/350] train loss : 1.6888 | train acc 77.86% | val loss 1.7169 | val acc 66.00% | time 0.268s\n",
      "[29/350] train loss : 1.6957 | train acc 77.14% | val loss 1.7132 | val acc 66.00% | time 0.268s\n",
      "[30/350] train loss : 1.6868 | train acc 77.86% | val loss 1.7094 | val acc 66.67% | time 0.270s\n",
      "[31/350] train loss : 1.6805 | train acc 77.14% | val loss 1.7056 | val acc 67.00% | time 0.268s\n",
      "[32/350] train loss : 1.6743 | train acc 77.14% | val loss 1.7016 | val acc 66.00% | time 0.270s\n",
      "[33/350] train loss : 1.6793 | train acc 77.86% | val loss 1.6976 | val acc 66.67% | time 0.269s\n",
      "[34/350] train loss : 1.6771 | train acc 76.43% | val loss 1.6936 | val acc 66.33% | time 0.269s\n",
      "[35/350] train loss : 1.6705 | train acc 74.29% | val loss 1.6896 | val acc 66.33% | time 0.270s\n",
      "[36/350] train loss : 1.6486 | train acc 75.71% | val loss 1.6855 | val acc 66.67% | time 0.271s\n",
      "[37/350] train loss : 1.6494 | train acc 77.86% | val loss 1.6813 | val acc 66.67% | time 0.270s\n",
      "[38/350] train loss : 1.6439 | train acc 77.14% | val loss 1.6769 | val acc 66.67% | time 0.271s\n",
      "[39/350] train loss : 1.6474 | train acc 76.43% | val loss 1.6725 | val acc 66.33% | time 0.271s\n",
      "[40/350] train loss : 1.6363 | train acc 75.00% | val loss 1.6680 | val acc 66.33% | time 0.270s\n",
      "[41/350] train loss : 1.6283 | train acc 73.57% | val loss 1.6635 | val acc 66.33% | time 0.270s\n",
      "[42/350] train loss : 1.6359 | train acc 70.00% | val loss 1.6588 | val acc 66.33% | time 0.270s\n",
      "[43/350] train loss : 1.6089 | train acc 74.29% | val loss 1.6541 | val acc 66.67% | time 0.271s\n",
      "[44/350] train loss : 1.6119 | train acc 77.14% | val loss 1.6493 | val acc 66.67% | time 0.270s\n",
      "[45/350] train loss : 1.6201 | train acc 73.57% | val loss 1.6446 | val acc 66.67% | time 0.270s\n",
      "[46/350] train loss : 1.6164 | train acc 75.00% | val loss 1.6397 | val acc 66.67% | time 0.270s\n",
      "[47/350] train loss : 1.5949 | train acc 76.43% | val loss 1.6348 | val acc 66.67% | time 0.269s\n",
      "[48/350] train loss : 1.5868 | train acc 77.14% | val loss 1.6298 | val acc 66.33% | time 0.270s\n",
      "[49/350] train loss : 1.5928 | train acc 74.29% | val loss 1.6246 | val acc 66.33% | time 0.270s\n",
      "[50/350] train loss : 1.5969 | train acc 72.14% | val loss 1.6195 | val acc 66.67% | time 0.269s\n",
      "[51/350] train loss : 1.5710 | train acc 77.86% | val loss 1.6142 | val acc 66.67% | time 0.270s\n",
      "[52/350] train loss : 1.5593 | train acc 77.86% | val loss 1.6090 | val acc 66.67% | time 0.271s\n",
      "[53/350] train loss : 1.5563 | train acc 78.57% | val loss 1.6036 | val acc 66.67% | time 0.270s\n",
      "[54/350] train loss : 1.5441 | train acc 76.43% | val loss 1.5980 | val acc 66.67% | time 0.270s\n",
      "[55/350] train loss : 1.5440 | train acc 72.86% | val loss 1.5924 | val acc 66.67% | time 0.271s\n",
      "[56/350] train loss : 1.5412 | train acc 76.43% | val loss 1.5868 | val acc 67.00% | time 0.270s\n",
      "[57/350] train loss : 1.5331 | train acc 72.86% | val loss 1.5811 | val acc 67.00% | time 0.270s\n",
      "[58/350] train loss : 1.5296 | train acc 77.86% | val loss 1.5754 | val acc 67.00% | time 0.269s\n",
      "[59/350] train loss : 1.5188 | train acc 75.71% | val loss 1.5696 | val acc 67.00% | time 0.270s\n",
      "[60/350] train loss : 1.5030 | train acc 73.57% | val loss 1.5638 | val acc 67.00% | time 0.271s\n",
      "[61/350] train loss : 1.4925 | train acc 72.86% | val loss 1.5579 | val acc 67.00% | time 0.268s\n",
      "[62/350] train loss : 1.4814 | train acc 77.14% | val loss 1.5519 | val acc 67.00% | time 0.271s\n",
      "[63/350] train loss : 1.4906 | train acc 72.86% | val loss 1.5459 | val acc 67.00% | time 0.268s\n",
      "[64/350] train loss : 1.4852 | train acc 75.00% | val loss 1.5398 | val acc 67.00% | time 0.267s\n",
      "[65/350] train loss : 1.4667 | train acc 75.00% | val loss 1.5336 | val acc 67.00% | time 0.268s\n",
      "[66/350] train loss : 1.4631 | train acc 76.43% | val loss 1.5274 | val acc 67.00% | time 0.269s\n",
      "[67/350] train loss : 1.4480 | train acc 70.71% | val loss 1.5211 | val acc 67.00% | time 0.268s\n",
      "[68/350] train loss : 1.4473 | train acc 77.14% | val loss 1.5148 | val acc 67.00% | time 0.268s\n",
      "[69/350] train loss : 1.4513 | train acc 73.57% | val loss 1.5084 | val acc 67.00% | time 0.267s\n",
      "[70/350] train loss : 1.4196 | train acc 72.86% | val loss 1.5019 | val acc 67.00% | time 0.268s\n",
      "[71/350] train loss : 1.4372 | train acc 74.29% | val loss 1.4954 | val acc 67.00% | time 0.270s\n",
      "[72/350] train loss : 1.3962 | train acc 76.43% | val loss 1.4889 | val acc 67.00% | time 0.270s\n",
      "[73/350] train loss : 1.3901 | train acc 75.71% | val loss 1.4824 | val acc 67.00% | time 0.272s\n",
      "[74/350] train loss : 1.3953 | train acc 77.86% | val loss 1.4758 | val acc 67.00% | time 0.272s\n",
      "[75/350] train loss : 1.3695 | train acc 77.14% | val loss 1.4692 | val acc 67.00% | time 0.271s\n",
      "[76/350] train loss : 1.3808 | train acc 76.43% | val loss 1.4626 | val acc 67.00% | time 0.271s\n",
      "[77/350] train loss : 1.3559 | train acc 77.14% | val loss 1.4560 | val acc 67.00% | time 0.271s\n",
      "[78/350] train loss : 1.3618 | train acc 74.29% | val loss 1.4493 | val acc 66.67% | time 0.272s\n",
      "[79/350] train loss : 1.3516 | train acc 73.57% | val loss 1.4426 | val acc 67.00% | time 0.271s\n",
      "[80/350] train loss : 1.3170 | train acc 74.29% | val loss 1.4359 | val acc 67.00% | time 0.271s\n",
      "[81/350] train loss : 1.3252 | train acc 74.29% | val loss 1.4291 | val acc 67.00% | time 0.272s\n",
      "[82/350] train loss : 1.3059 | train acc 74.29% | val loss 1.4223 | val acc 67.00% | time 0.272s\n",
      "[83/350] train loss : 1.3122 | train acc 75.71% | val loss 1.4155 | val acc 67.00% | time 0.271s\n",
      "[84/350] train loss : 1.3073 | train acc 74.29% | val loss 1.4088 | val acc 67.00% | time 0.273s\n",
      "[85/350] train loss : 1.2735 | train acc 75.71% | val loss 1.4019 | val acc 67.00% | time 0.272s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86/350] train loss : 1.2756 | train acc 76.43% | val loss 1.3950 | val acc 67.00% | time 0.272s\n",
      "[87/350] train loss : 1.2552 | train acc 77.14% | val loss 1.3881 | val acc 67.00% | time 0.273s\n",
      "[88/350] train loss : 1.2473 | train acc 74.29% | val loss 1.3811 | val acc 67.00% | time 0.271s\n",
      "[89/350] train loss : 1.2280 | train acc 77.86% | val loss 1.3742 | val acc 67.00% | time 0.271s\n",
      "[90/350] train loss : 1.2122 | train acc 74.29% | val loss 1.3674 | val acc 67.00% | time 0.271s\n",
      "[91/350] train loss : 1.2366 | train acc 75.00% | val loss 1.3606 | val acc 67.00% | time 0.271s\n",
      "[92/350] train loss : 1.2184 | train acc 76.43% | val loss 1.3538 | val acc 66.67% | time 0.271s\n",
      "[93/350] train loss : 1.2090 | train acc 77.86% | val loss 1.3469 | val acc 66.67% | time 0.271s\n",
      "[94/350] train loss : 1.1979 | train acc 78.57% | val loss 1.3400 | val acc 66.67% | time 0.271s\n",
      "[95/350] train loss : 1.1811 | train acc 74.29% | val loss 1.3333 | val acc 66.67% | time 0.272s\n",
      "[96/350] train loss : 1.1582 | train acc 76.43% | val loss 1.3266 | val acc 66.67% | time 0.272s\n",
      "[97/350] train loss : 1.1515 | train acc 77.14% | val loss 1.3199 | val acc 66.67% | time 0.271s\n",
      "[98/350] train loss : 1.1435 | train acc 74.29% | val loss 1.3134 | val acc 66.67% | time 0.271s\n",
      "[99/350] train loss : 1.1402 | train acc 76.43% | val loss 1.3069 | val acc 66.67% | time 0.272s\n",
      "[100/350] train loss : 1.1431 | train acc 75.71% | val loss 1.3006 | val acc 66.67% | time 0.272s\n",
      "[101/350] train loss : 1.0985 | train acc 74.29% | val loss 1.2944 | val acc 66.67% | time 0.273s\n",
      "[102/350] train loss : 1.1086 | train acc 73.57% | val loss 1.2882 | val acc 66.67% | time 0.271s\n",
      "[103/350] train loss : 1.1416 | train acc 72.86% | val loss 1.2821 | val acc 66.67% | time 0.272s\n",
      "[104/350] train loss : 1.0702 | train acc 76.43% | val loss 1.2759 | val acc 67.00% | time 0.270s\n",
      "[105/350] train loss : 1.0363 | train acc 78.57% | val loss 1.2699 | val acc 67.33% | time 0.270s\n",
      "[106/350] train loss : 1.0914 | train acc 77.14% | val loss 1.2640 | val acc 67.67% | time 0.275s\n",
      "[107/350] train loss : 1.0368 | train acc 77.86% | val loss 1.2580 | val acc 67.67% | time 0.269s\n",
      "[108/350] train loss : 1.0275 | train acc 77.14% | val loss 1.2520 | val acc 67.67% | time 0.267s\n",
      "[109/350] train loss : 1.0443 | train acc 77.86% | val loss 1.2463 | val acc 67.67% | time 0.267s\n",
      "[110/350] train loss : 1.0295 | train acc 77.86% | val loss 1.2407 | val acc 68.00% | time 0.267s\n",
      "[111/350] train loss : 0.9853 | train acc 80.71% | val loss 1.2354 | val acc 68.00% | time 0.268s\n",
      "[112/350] train loss : 1.0033 | train acc 77.14% | val loss 1.2300 | val acc 68.00% | time 0.266s\n",
      "[113/350] train loss : 1.0065 | train acc 75.00% | val loss 1.2249 | val acc 68.00% | time 0.266s\n",
      "[114/350] train loss : 1.0163 | train acc 75.71% | val loss 1.2200 | val acc 69.00% | time 0.267s\n",
      "[115/350] train loss : 0.9730 | train acc 80.00% | val loss 1.2152 | val acc 69.67% | time 0.267s\n",
      "[116/350] train loss : 1.0229 | train acc 77.86% | val loss 1.2106 | val acc 69.67% | time 0.265s\n",
      "[117/350] train loss : 1.0033 | train acc 74.29% | val loss 1.2063 | val acc 69.67% | time 0.267s\n",
      "[118/350] train loss : 0.9866 | train acc 77.14% | val loss 1.2023 | val acc 69.67% | time 0.266s\n",
      "[119/350] train loss : 0.9743 | train acc 78.57% | val loss 1.1982 | val acc 69.67% | time 0.267s\n",
      "[120/350] train loss : 0.9701 | train acc 77.14% | val loss 1.1939 | val acc 70.00% | time 0.268s\n",
      "[121/350] train loss : 0.9894 | train acc 74.29% | val loss 1.1897 | val acc 70.00% | time 0.268s\n",
      "[122/350] train loss : 0.9717 | train acc 79.29% | val loss 1.1856 | val acc 70.33% | time 0.267s\n",
      "[123/350] train loss : 0.9199 | train acc 78.57% | val loss 1.1817 | val acc 70.67% | time 0.267s\n",
      "[124/350] train loss : 0.9160 | train acc 75.71% | val loss 1.1780 | val acc 70.67% | time 0.266s\n",
      "[125/350] train loss : 0.9513 | train acc 81.43% | val loss 1.1747 | val acc 70.67% | time 0.266s\n",
      "[126/350] train loss : 0.9346 | train acc 78.57% | val loss 1.1716 | val acc 70.67% | time 0.268s\n",
      "[127/350] train loss : 0.9259 | train acc 79.29% | val loss 1.1685 | val acc 70.67% | time 0.267s\n",
      "[128/350] train loss : 0.9320 | train acc 77.14% | val loss 1.1655 | val acc 70.67% | time 0.266s\n",
      "[129/350] train loss : 0.8845 | train acc 78.57% | val loss 1.1622 | val acc 70.67% | time 0.266s\n",
      "[130/350] train loss : 0.9314 | train acc 77.86% | val loss 1.1587 | val acc 70.67% | time 0.267s\n",
      "[131/350] train loss : 0.9110 | train acc 77.14% | val loss 1.1551 | val acc 70.67% | time 0.267s\n",
      "[132/350] train loss : 0.9178 | train acc 79.29% | val loss 1.1518 | val acc 70.67% | time 0.267s\n",
      "[133/350] train loss : 0.9026 | train acc 79.29% | val loss 1.1488 | val acc 70.67% | time 0.266s\n",
      "[134/350] train loss : 0.8922 | train acc 77.86% | val loss 1.1460 | val acc 70.67% | time 0.267s\n",
      "[135/350] train loss : 0.8837 | train acc 78.57% | val loss 1.1432 | val acc 71.00% | time 0.266s\n",
      "[136/350] train loss : 0.8607 | train acc 80.71% | val loss 1.1406 | val acc 71.00% | time 0.268s\n",
      "[137/350] train loss : 0.8367 | train acc 79.29% | val loss 1.1378 | val acc 71.00% | time 0.266s\n",
      "[138/350] train loss : 0.8487 | train acc 80.00% | val loss 1.1351 | val acc 71.00% | time 0.266s\n",
      "[139/350] train loss : 0.8571 | train acc 79.29% | val loss 1.1326 | val acc 71.33% | time 0.266s\n",
      "[140/350] train loss : 0.8294 | train acc 82.14% | val loss 1.1302 | val acc 71.33% | time 0.267s\n",
      "[141/350] train loss : 0.8547 | train acc 80.00% | val loss 1.1279 | val acc 71.33% | time 0.267s\n",
      "[142/350] train loss : 0.8302 | train acc 75.71% | val loss 1.1257 | val acc 71.33% | time 0.267s\n",
      "[143/350] train loss : 0.8678 | train acc 80.00% | val loss 1.1234 | val acc 71.33% | time 0.265s\n",
      "[144/350] train loss : 0.8601 | train acc 80.71% | val loss 1.1213 | val acc 71.33% | time 0.267s\n",
      "[145/350] train loss : 0.8783 | train acc 78.57% | val loss 1.1190 | val acc 71.33% | time 0.265s\n",
      "[146/350] train loss : 0.8253 | train acc 80.71% | val loss 1.1168 | val acc 71.33% | time 0.267s\n",
      "[147/350] train loss : 0.8568 | train acc 79.29% | val loss 1.1150 | val acc 71.33% | time 0.266s\n",
      "[148/350] train loss : 0.8241 | train acc 82.86% | val loss 1.1133 | val acc 71.33% | time 0.266s\n",
      "[149/350] train loss : 0.7645 | train acc 81.43% | val loss 1.1113 | val acc 71.33% | time 0.267s\n",
      "[150/350] train loss : 0.7884 | train acc 80.71% | val loss 1.1092 | val acc 71.33% | time 0.266s\n",
      "[151/350] train loss : 0.7838 | train acc 77.86% | val loss 1.1070 | val acc 71.33% | time 0.265s\n",
      "[152/350] train loss : 0.8035 | train acc 79.29% | val loss 1.1049 | val acc 71.33% | time 0.267s\n",
      "[153/350] train loss : 0.7678 | train acc 82.86% | val loss 1.1027 | val acc 71.67% | time 0.266s\n",
      "[154/350] train loss : 0.7984 | train acc 78.57% | val loss 1.1005 | val acc 71.67% | time 0.265s\n",
      "[155/350] train loss : 0.7715 | train acc 80.71% | val loss 1.0984 | val acc 71.67% | time 0.266s\n",
      "[156/350] train loss : 0.7912 | train acc 82.14% | val loss 1.0964 | val acc 71.67% | time 0.268s\n",
      "[157/350] train loss : 0.7913 | train acc 80.00% | val loss 1.0945 | val acc 71.33% | time 0.266s\n",
      "[158/350] train loss : 0.7977 | train acc 81.43% | val loss 1.0932 | val acc 71.67% | time 0.268s\n",
      "[159/350] train loss : 0.7719 | train acc 81.43% | val loss 1.0918 | val acc 71.67% | time 0.268s\n",
      "[160/350] train loss : 0.7891 | train acc 81.43% | val loss 1.0904 | val acc 71.67% | time 0.267s\n",
      "[161/350] train loss : 0.7900 | train acc 80.00% | val loss 1.0892 | val acc 71.67% | time 0.267s\n",
      "[162/350] train loss : 0.7450 | train acc 84.29% | val loss 1.0879 | val acc 71.67% | time 0.267s\n",
      "[163/350] train loss : 0.7352 | train acc 82.14% | val loss 1.0864 | val acc 71.67% | time 0.267s\n",
      "[164/350] train loss : 0.7779 | train acc 81.43% | val loss 1.0849 | val acc 71.67% | time 0.267s\n",
      "[165/350] train loss : 0.7985 | train acc 80.71% | val loss 1.0833 | val acc 71.67% | time 0.267s\n",
      "[166/350] train loss : 0.7569 | train acc 82.14% | val loss 1.0820 | val acc 71.67% | time 0.267s\n",
      "[167/350] train loss : 0.7524 | train acc 81.43% | val loss 1.0805 | val acc 71.67% | time 0.268s\n",
      "[168/350] train loss : 0.7895 | train acc 80.00% | val loss 1.0791 | val acc 71.67% | time 0.268s\n",
      "[169/350] train loss : 0.7720 | train acc 81.43% | val loss 1.0778 | val acc 71.67% | time 0.269s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170/350] train loss : 0.7544 | train acc 83.57% | val loss 1.0762 | val acc 71.67% | time 0.269s\n",
      "[171/350] train loss : 0.7637 | train acc 83.57% | val loss 1.0750 | val acc 71.67% | time 0.269s\n",
      "[172/350] train loss : 0.8260 | train acc 81.43% | val loss 1.0742 | val acc 71.33% | time 0.267s\n",
      "[173/350] train loss : 0.7588 | train acc 81.43% | val loss 1.0739 | val acc 71.00% | time 0.267s\n",
      "[174/350] train loss : 0.7507 | train acc 82.14% | val loss 1.0738 | val acc 71.00% | time 0.267s\n",
      "[175/350] train loss : 0.7851 | train acc 82.86% | val loss 1.0740 | val acc 71.00% | time 0.267s\n",
      "[176/350] train loss : 0.7618 | train acc 80.71% | val loss 1.0739 | val acc 71.33% | time 0.266s\n",
      "[177/350] train loss : 0.7373 | train acc 82.86% | val loss 1.0738 | val acc 71.33% | time 0.266s\n",
      "[178/350] train loss : 0.7811 | train acc 78.57% | val loss 1.0737 | val acc 71.33% | time 0.268s\n",
      "[179/350] train loss : 0.7160 | train acc 84.29% | val loss 1.0734 | val acc 71.33% | time 0.267s\n",
      "[180/350] train loss : 0.7491 | train acc 80.71% | val loss 1.0731 | val acc 71.33% | time 0.267s\n",
      "[181/350] train loss : 0.7980 | train acc 82.14% | val loss 1.0724 | val acc 71.33% | time 0.266s\n",
      "[182/350] train loss : 0.7392 | train acc 83.57% | val loss 1.0717 | val acc 71.33% | time 0.267s\n",
      "[183/350] train loss : 0.7330 | train acc 83.57% | val loss 1.0709 | val acc 71.33% | time 0.267s\n",
      "[184/350] train loss : 0.6922 | train acc 82.86% | val loss 1.0703 | val acc 71.00% | time 0.267s\n",
      "[185/350] train loss : 0.7210 | train acc 82.14% | val loss 1.0696 | val acc 71.00% | time 0.267s\n",
      "[186/350] train loss : 0.6989 | train acc 85.00% | val loss 1.0689 | val acc 71.00% | time 0.267s\n",
      "[187/350] train loss : 0.7238 | train acc 81.43% | val loss 1.0683 | val acc 71.00% | time 0.266s\n",
      "[188/350] train loss : 0.7115 | train acc 79.29% | val loss 1.0676 | val acc 71.00% | time 0.267s\n",
      "[189/350] train loss : 0.6881 | train acc 82.14% | val loss 1.0670 | val acc 71.00% | time 0.267s\n",
      "[190/350] train loss : 0.7693 | train acc 84.29% | val loss 1.0664 | val acc 70.67% | time 0.266s\n",
      "[191/350] train loss : 0.7036 | train acc 82.86% | val loss 1.0657 | val acc 71.33% | time 0.266s\n",
      "[192/350] train loss : 0.7052 | train acc 83.57% | val loss 1.0650 | val acc 71.67% | time 0.267s\n",
      "[193/350] train loss : 0.7170 | train acc 85.00% | val loss 1.0643 | val acc 71.67% | time 0.267s\n",
      "[194/350] train loss : 0.7209 | train acc 82.86% | val loss 1.0639 | val acc 71.67% | time 0.269s\n",
      "[195/350] train loss : 0.7524 | train acc 84.29% | val loss 1.0638 | val acc 71.67% | time 0.269s\n",
      "[196/350] train loss : 0.6762 | train acc 85.00% | val loss 1.0633 | val acc 71.67% | time 0.268s\n",
      "[197/350] train loss : 0.7051 | train acc 83.57% | val loss 1.0623 | val acc 71.33% | time 0.269s\n",
      "[198/350] train loss : 0.7257 | train acc 80.71% | val loss 1.0614 | val acc 71.33% | time 0.267s\n",
      "[199/350] train loss : 0.7656 | train acc 82.86% | val loss 1.0606 | val acc 71.33% | time 0.268s\n",
      "[200/350] train loss : 0.7048 | train acc 83.57% | val loss 1.0594 | val acc 71.33% | time 0.268s\n",
      "[201/350] train loss : 0.7313 | train acc 82.86% | val loss 1.0582 | val acc 71.33% | time 0.268s\n",
      "[202/350] train loss : 0.6816 | train acc 82.14% | val loss 1.0569 | val acc 71.33% | time 0.268s\n",
      "[203/350] train loss : 0.6819 | train acc 85.71% | val loss 1.0559 | val acc 71.33% | time 0.267s\n",
      "[204/350] train loss : 0.7140 | train acc 80.71% | val loss 1.0551 | val acc 71.33% | time 0.267s\n",
      "[205/350] train loss : 0.7061 | train acc 84.29% | val loss 1.0542 | val acc 71.33% | time 0.266s\n",
      "[206/350] train loss : 0.7303 | train acc 82.14% | val loss 1.0532 | val acc 71.33% | time 0.266s\n",
      "[207/350] train loss : 0.6886 | train acc 83.57% | val loss 1.0520 | val acc 71.33% | time 0.265s\n",
      "[208/350] train loss : 0.7389 | train acc 80.00% | val loss 1.0508 | val acc 71.33% | time 0.265s\n",
      "[209/350] train loss : 0.7156 | train acc 82.86% | val loss 1.0496 | val acc 71.67% | time 0.267s\n",
      "[210/350] train loss : 0.6854 | train acc 82.14% | val loss 1.0487 | val acc 71.67% | time 0.266s\n",
      "[211/350] train loss : 0.7083 | train acc 81.43% | val loss 1.0479 | val acc 71.33% | time 0.265s\n",
      "[212/350] train loss : 0.6626 | train acc 85.00% | val loss 1.0470 | val acc 71.33% | time 0.266s\n",
      "[213/350] train loss : 0.6583 | train acc 83.57% | val loss 1.0462 | val acc 71.33% | time 0.265s\n",
      "[214/350] train loss : 0.6916 | train acc 82.14% | val loss 1.0455 | val acc 71.33% | time 0.265s\n",
      "[215/350] train loss : 0.6946 | train acc 84.29% | val loss 1.0443 | val acc 71.33% | time 0.265s\n",
      "[216/350] train loss : 0.6505 | train acc 83.57% | val loss 1.0431 | val acc 71.33% | time 0.268s\n",
      "[217/350] train loss : 0.7115 | train acc 83.57% | val loss 1.0417 | val acc 71.00% | time 0.266s\n",
      "[218/350] train loss : 0.6638 | train acc 84.29% | val loss 1.0405 | val acc 71.00% | time 0.266s\n",
      "[219/350] train loss : 0.6928 | train acc 82.14% | val loss 1.0393 | val acc 71.00% | time 0.266s\n",
      "[220/350] train loss : 0.7197 | train acc 80.71% | val loss 1.0384 | val acc 71.00% | time 0.265s\n",
      "[221/350] train loss : 0.6630 | train acc 85.00% | val loss 1.0378 | val acc 71.00% | time 0.266s\n",
      "[222/350] train loss : 0.6530 | train acc 81.43% | val loss 1.0375 | val acc 71.00% | time 0.267s\n",
      "[223/350] train loss : 0.7166 | train acc 82.14% | val loss 1.0375 | val acc 71.00% | time 0.266s\n",
      "[224/350] train loss : 0.6883 | train acc 83.57% | val loss 1.0377 | val acc 71.00% | time 0.265s\n",
      "[225/350] train loss : 0.6797 | train acc 82.14% | val loss 1.0379 | val acc 71.00% | time 0.266s\n",
      "[226/350] train loss : 0.6354 | train acc 86.43% | val loss 1.0378 | val acc 71.00% | time 0.266s\n",
      "[227/350] train loss : 0.7157 | train acc 79.29% | val loss 1.0380 | val acc 71.00% | time 0.267s\n",
      "[228/350] train loss : 0.7297 | train acc 82.86% | val loss 1.0380 | val acc 71.00% | time 0.265s\n",
      "[229/350] train loss : 0.6838 | train acc 82.86% | val loss 1.0378 | val acc 71.00% | time 0.267s\n",
      "[230/350] train loss : 0.7034 | train acc 82.86% | val loss 1.0378 | val acc 71.00% | time 0.267s\n",
      "[231/350] train loss : 0.6577 | train acc 82.14% | val loss 1.0377 | val acc 71.33% | time 0.266s\n",
      "[232/350] train loss : 0.6227 | train acc 84.29% | val loss 1.0374 | val acc 71.33% | time 0.266s\n",
      "[233/350] train loss : 0.6450 | train acc 84.29% | val loss 1.0375 | val acc 71.00% | time 0.266s\n",
      "[234/350] train loss : 0.6656 | train acc 82.86% | val loss 1.0375 | val acc 71.00% | time 0.265s\n",
      "[235/350] train loss : 0.6530 | train acc 83.57% | val loss 1.0372 | val acc 71.00% | time 0.266s\n",
      "[236/350] train loss : 0.7043 | train acc 82.86% | val loss 1.0370 | val acc 71.00% | time 0.266s\n",
      "[237/350] train loss : 0.6261 | train acc 83.57% | val loss 1.0368 | val acc 71.33% | time 0.265s\n",
      "[238/350] train loss : 0.7071 | train acc 81.43% | val loss 1.0365 | val acc 71.33% | time 0.267s\n",
      "[239/350] train loss : 0.6047 | train acc 85.00% | val loss 1.0364 | val acc 71.33% | time 0.266s\n",
      "[240/350] train loss : 0.6782 | train acc 83.57% | val loss 1.0361 | val acc 71.33% | time 0.265s\n",
      "[241/350] train loss : 0.6436 | train acc 85.00% | val loss 1.0357 | val acc 71.33% | time 0.267s\n",
      "[242/350] train loss : 0.6401 | train acc 84.29% | val loss 1.0351 | val acc 71.33% | time 0.267s\n",
      "[243/350] train loss : 0.6193 | train acc 84.29% | val loss 1.0345 | val acc 71.33% | time 0.266s\n",
      "[244/350] train loss : 0.6698 | train acc 81.43% | val loss 1.0337 | val acc 71.33% | time 0.268s\n",
      "[245/350] train loss : 0.6235 | train acc 85.00% | val loss 1.0330 | val acc 71.00% | time 0.268s\n",
      "[246/350] train loss : 0.6835 | train acc 83.57% | val loss 1.0325 | val acc 71.33% | time 0.267s\n",
      "[247/350] train loss : 0.6686 | train acc 84.29% | val loss 1.0319 | val acc 71.33% | time 0.267s\n",
      "[248/350] train loss : 0.6586 | train acc 85.00% | val loss 1.0318 | val acc 71.33% | time 0.267s\n",
      "[249/350] train loss : 0.6126 | train acc 84.29% | val loss 1.0320 | val acc 71.00% | time 0.268s\n",
      "[250/350] train loss : 0.6701 | train acc 83.57% | val loss 1.0325 | val acc 71.00% | time 0.268s\n",
      "[251/350] train loss : 0.6638 | train acc 82.14% | val loss 1.0331 | val acc 71.33% | time 0.267s\n",
      "[252/350] train loss : 0.6385 | train acc 82.14% | val loss 1.0334 | val acc 71.33% | time 0.268s\n",
      "[253/350] train loss : 0.6723 | train acc 82.86% | val loss 1.0339 | val acc 71.33% | time 0.269s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[254/350] train loss : 0.6418 | train acc 83.57% | val loss 1.0342 | val acc 71.33% | time 0.267s\n",
      "[255/350] train loss : 0.6644 | train acc 83.57% | val loss 1.0344 | val acc 71.33% | time 0.268s\n",
      "[256/350] train loss : 0.6297 | train acc 85.71% | val loss 1.0345 | val acc 71.33% | time 0.267s\n",
      "[257/350] train loss : 0.6650 | train acc 79.29% | val loss 1.0346 | val acc 71.33% | time 0.268s\n",
      "[258/350] train loss : 0.6424 | train acc 85.00% | val loss 1.0346 | val acc 71.33% | time 0.267s\n",
      "There's no improvements during 10 epochs and so stop the training.\n",
      "Test Accuracy : 63.40\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time \n",
    "import random \n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torch.autograd import Variable\n",
    " \n",
    "    \n",
    "device = torch.device('cuda' if(torch.cuda.is_available()) else 'cpu')\n",
    "\n",
    "# load the data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# parameter intialization\n",
    "N = features.size(0) # num_of_nodes\n",
    "F_F = features.size(1) # num_of_features\n",
    "H = 4 # hidden nodes\n",
    "C = labels.max().item() + 1 # num_classes\n",
    "\n",
    "# for validation\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 10.\n",
    "\n",
    "# init training object\n",
    "network = GAT(F_F, H, C, 0.5, 0.2, 3).to(device)\n",
    "adj = adj.to(device)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.005, weight_decay = 5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "\n",
    "# Train\n",
    "for epoch in range(350):\n",
    "    t = time.time()\n",
    "    network.train()\n",
    "\n",
    "    preds = network(features, adj) \n",
    "    train_loss = criterion(preds[idx_train], labels[idx_train])\n",
    "    train_acc = accuracy(preds[idx_train], labels[idx_train])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        preds_val = network(features, adj)\n",
    "        val_loss = criterion(preds_val[idx_val], labels[idx_val])\n",
    "        val_acc = accuracy(preds_val[idx_val], labels[idx_val])\n",
    "\n",
    "        # early stopping\n",
    "        if val_loss < best_loss :\n",
    "            best_loss = val_loss\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    train_accs.append(train_acc.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    val_accs.append(val_acc.item())\n",
    "\n",
    "    print('[%d/%d] train loss : %.4f | train acc %.2f%% | val loss %.4f | val acc %.2f%% | time %.3fs'\n",
    "                %(epoch+1, 350, train_loss.item(), train_acc.item() * 100, val_loss.item(), val_acc.item() * 100, time.time() - t))\n",
    "\n",
    "    if epochs_since_improvement > 10 - 1 :\n",
    "        print(\"There's no improvements during %d epochs and so stop the training.\"%(10))\n",
    "        break\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    network.eval()\n",
    "    preds = network(features, adj)\n",
    "    test_acc = accuracy(preds[idx_test], labels[idx_test])\n",
    "    print('Test Accuracy : %.2f'%(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
